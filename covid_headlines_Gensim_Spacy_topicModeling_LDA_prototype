# -*- coding: utf-8 -*-
"""
Created on Fri Jan 19 17:49:19 2021
Notes: what´s the deal with the article?????
@author: Eric
"""
 
import spacy
import gensim
from collections import defaultdict
from gensim.corpora import Dictionary
from spacy import displacy
from pathlib import Path
from gensim.models import LdaModel
 
text= open("Covid_headlines.txt",encoding="utf-8").read() 
nlp=spacy.load("es_core_news_lg")
 
# Cleaning the input data
my_stop_words = [u'decir', u'\'s', u'mr', u'ser', u'dicho', "\n\n","asimismo"]
for stopword in my_stop_words:
    lexeme = nlp.vocab[stopword]
    lexeme.is_stop = True
    
nlp.Defaults.stop_words -= {"no", "empleo", }
 
doc = nlp(text.lower())
 
# we add some words to the stop word list
texts, article = [], []
for w in doc:
    # if it's not a stop word or punctuation mark, add it to our article!
    if w.text != '\n\n' and not w.is_stop and not w.is_punct and not w.like_num and w.text != 'yo':
        print("text/pos/tag>>>>",w.text, w.pos_,w.tag)
        # we add the lematized version of the word
        article.append(w.lemma_)
    # if it's a new line, it means we're onto our next document
    if w.text == '\n\n':
        texts.append(article)
        article = []
        
# Name Entity Recognition
for ent in doc.ents:
   print("NER>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>",ent.text, ent.label_)
# Bigrams
bigram_model = gensim.models.Phrases(texts)
texts=[bigram_model[line] for line in texts]
#texts[0:20]
 
for ngrams, _ in bigram_model.vocab.items():# to inspect the collocations found with the bigram object
    unicode_ngrams = ngrams.decode('utf-8')
    if '_' in unicode_ngrams:
        print("GRAM>>>>>>>>>>",unicode_ngrams)
        
# Name Entity Recognition visualization
doc.user_data["title"] = """Name Entity Recognition visualization 
                                 applied to the covid_NER_headlines dataset 
                                                scraped from Tadeo University"""
displacy.serve(doc, style="ent")
        
# remove words that appear only once
frequency = defaultdict(int)
for text in texts:
    for token in text:
        frequency[token] += 1
 
texts = [[token for token in text if frequency[token] > 1]for text in texts]        
# vectorization
dictionary = Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
# dictionary.save_as_text('gensimDictionary.txt')
# corpus[100]
dictionary.save_as_text('covid_headlines_colombia_dictionary.txt')
 
# Training parameters
num_topics = 10
chunksize = 2000
passes = 30
iterations = 400
eval_every = 10  #  evaluating model perplexity takes too long. you can avoid it by passing None
random_state=100
alpha='symmetric' # auto and asymmetric are the other options
#Training the model
lda_model = LdaModel(
    corpus=corpus,
    id2word=dictionary,
    chunksize=chunksize,
    alpha=alpha,
    eta='auto',
    minimum_probability=0.01,
    iterations=iterations,
    num_topics=num_topics,
    passes=passes,
    random_state=random_state,
    eval_every=eval_every
)
print("LDA")
 
#parameters for show_topics
log=True # Whether the output is also logged, besides being returned.
num_words=5 #Number of words to be presented for each topic. These will be the most relevant words (assigned the highest probability for each topic).
print(lda_model.show_topics(num_words=num_words,log=log )) 
print( "Voilà, your yopic model is ready")
 
#Dependency Visualization
svg = displacy.render([doc], style="dep", jupyter=False)
output_path = Path("Covid_Headlines_Colombia_012421_nlp_dependency_visualization.svg")
output_path.open("w", encoding="utf-8").write(svg) 
 
#Remember, texts contain the tokenized and cleaned version of the original text data
#the corpus is our bag of words representation
#and in case you want to inspect the data use the following:
 
texts [0][0:10]
corpus
corpus[0][0:10]
  
#dictionary.doc2bow(["covid"])
#dictionary.token2id["covid"]
#dictionary[19]
#print("list of all texts>>>>",texts)
#print("first text>>>>",texts[0])
#Note: To see the current set of stopwords, use:
#print(nlp.Defaults.stop_words) 
